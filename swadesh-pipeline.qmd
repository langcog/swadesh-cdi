---
title: "Swadesh CDI pipeline"
format: html
---

```{r}
library(wordbankr)
library(tidyverse)
library(glue)
library(mirt)
library(RColorBrewer)
require(gplots)
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)

S_LEN = 100 # Swadesh list length
N_RAND = 100 # number of random sublists to compare to
BEST_K = 25 # best for 2 strata and unstratified; k=26 for category
BEST_Ks = 23:29
```

# Data fetching and stitching
Get new form definitions with unified items
```{r}
languages <- list.files("data/new_items") |> str_sub(end = -5)
pronouns <- read_csv("data/pronouns.csv") |> 
  mutate(uid = glue("{form}_{itemID}"))

rep_items <- list()

# Validate to ensure no repeated items
for (language in languages) {
  ni <- get_new_items(language, pronouns)
  forms <- ni$forms
  new_items <- ni$new_items
  rep_ids <- new_items |> 
    select(category, definition) |> 
    duplicated()
  repeated <- new_items[rep_ids,]
  
  if (nrow(repeated) > 0) {
    repeated <- repeated |> 
      mutate(language = language)
    rep_items <- c(rep_items, list(repeated))
  }
}

# get syntactic <-> semantic category mapping
syn_cats <- read_csv("data/categories.csv",
                     col_names = c("category", "lexical_category", "lexical_class")) |> 
  distinct()
```

Stitch data together and save outputs
```{r, eval=F}
for (language in languages) {
  ni <- get_new_items(language, pronouns)
  df <- make_data(language, ni$forms, ni$new_items)
  
  output <- list(all_demo = df$all_demo,
                 items = ni$new_items,
                 all_long = df$all_long,
                 all_prod = df$all_prod)
  
  saveRDS(output, glue("data/all_forms/{language}_data.rds"))
}
```


# IRT modelling and parameter extraction
Run 2PL models
```{r}
languages <- list.files("data/all_forms") |> str_sub(end = -10)
```

```{r eval=F}
completed <- list.files("data/prod_models") |> str_sub(end = -28)

mirtCluster()
for (language in setdiff(languages, completed)) {
  run_2PL_model(language)
}
```

Get demographics
ToDo: update or remove
```{r, eval=F}
demog <- lapply(languages, \(language) {
  lang_data <- readRDS(glue("data/all_forms/{language}_data.rds"))
  # FIXME: all_demo is wrong---maybe because of filter_age
  lang_data$all_long |> 
    select(language, data_id, form_type) |> 
    distinct() |> 
    count(language, form_type)
}) |> bind_rows()

included <- sapply(languages, \(language) {
  model <- readRDS(glue("data/prod_models/{language}_2PL_allforms_prod_fits.rds"))
  model$model@Data$data |> nrow()
})

demog <- demog |> 
  pivot_wider(names_from = form_type,
              values_from = n) |> 
  mutate(included = included)

saveRDS(demog, "data/intermediates/demographics.rds")
```


Collapse cross-linguistic item parameters
```{r eval=F}
xldf <- list()
full_fscores <- list()
n_subj <- c()
n_item <- c()
n_iter <- c()

for (language in languages) {
  lang_data <- readRDS(glue("data/all_forms/{language}_data.rds"))
  fitted <- readRDS(glue("data/prod_models/{language}_2PL_allforms_prod_fits.rds"))
  n_subj <- c(n_subj, nrow(fitted$model@Data$data))
  n_item <- c(n_item, ncol(fitted$model@Data$data))
  n_iter <- c(n_iter, fitted$model@OptimInfo$iter)
    
  lang_fscores <- data.frame(fscores(fitted$model, 
                                     method = "MAP",
                                     full.scores = F)) 
  full_fscores[[language]] <- tibble(full_theta = lang_fscores$G,
                                     full_sumscore = rowSums(lang_fscores |> select(-G, -SE_G), na.rm=T))
  # could also pull age from lang_data$all_demo
  df <- fitted$coefs |> 
    rename("uid" = "definition") |> 
    left_join(lang_data$items |> 
                select(uid, category, definition, gloss, uni_lemma) |> # ToDo: want lexical_category, but it's not here...
                mutate(uid = str_replace(uid, " ", ".")),
              by = "uid") |> 
    mutate(language = language)
  xldf <- c(xldf, list(df))
}
xldf <- bind_rows(xldf)
saveRDS(xldf, "data/intermediates/xldf_prod_allforms.rds")
saveRDS(full_fscores, "data/intermediates/full_fscores_allforms.rds")
# xx <- bind_rows(full_fscores)
# plot(xx$full_theta, xx$full_sumscore) # cor = .71

model_stats <- tibble(language = languages,
                      participants = n_subj,
                      items = n_item,
                      iterations = n_iter)
saveRDS(model_stats, "data/intermediates/model_stats.rds")
```

```{r, load-irt-parameters}
xldf <- readRDS("data/intermediates/xldf_prod_allforms.rds")
model_stats <- readRDS("data/intermediates/model_stats.rds")
```

Clean IRT parameters (`xldf`)
```{r}
xldf_clean <- xldf |> 
  filter(!is.na(uni_lemma), !is.na(d)) |> 
  mutate(category = case_when(
    uni_lemma == "yes" & language == "Slovak" ~ "games_routines",
    uni_lemma == "no" & language == "Slovak" ~ "games_routines",
    uni_lemma == "peekaboo" & language == "Slovak" ~ "games_routines",
    uni_lemma == "finished" & language == "Slovak" ~ "games_routines",
    uni_lemma == "hello" & language == "Slovak" ~ "games_routines",
    uni_lemma == "bye" & language == "Slovak" ~ "games_routines",
    uni_lemma == "be" & language == "Slovak" ~ "helping_verbs",
    category == "descriptive_words (adjectives)" ~ "descriptive_words",
    category == "descriptive_words (adverbs)" ~ "descriptive_words",
    category == "outside_places" ~ "outside", 
    category == "places" ~ "outside", # combine outside and places
    category == "articles" ~ "quantifiers",
    category == "hold" ~ "household",
    category == "states" ~ "action_words",
    category == "prepositions" ~ "locations",
    category == "mental" & uni_lemma == "understand" ~ "action_words",
    category == "mental" & uni_lemma == "remember" ~ "action_words",
    category == "mental" & uni_lemma == "mad" ~ "descriptive_words",
    category == "negation_words" ~ "games_routines", # Arabic: 'finished', 'don't want', 'not mine'...
    uni_lemma == "although" & is.na(category) ~ "connecting_words",
    uni_lemma == "accident" & is.na(category) ~ "other",
    uni_lemma == "expensive" & is.na(category) ~ "descriptive_words",
    uni_lemma == "album" & category == "toys" ~ "household",
    uni_lemma == "allowed" & category == "games_routines" ~ "helping_verbs",
    uni_lemma == "bored" & is.na(category) ~ "descriptive_words",
    uni_lemma == "circle" & is.na(category) ~ "descriptive_words",
    uni_lemma == "backpack" ~ "household",
    uni_lemma == "only" ~ "quantifiers",
    uni_lemma == "material" & is.na(category) ~ "household",
    uni_lemma == "microscope" & is.na(category) ~ "toys",
    uni_lemma == "mall" ~ "outside",
    uni_lemma == "our" ~ "pronouns",
    uni_lemma == "I" ~ "pronouns",
    uni_lemma == "today" ~ "time_words",
    uni_lemma == "now" ~ "time_words",
    uni_lemma == "bounce" ~ "action_words",
    uni_lemma == "blouse" ~ "clothing",
    uni_lemma == "gas" ~ "outside",
    uni_lemma == "yet" ~ "descriptive_words",
    uni_lemma == "deep" & is.na(category) ~ "descriptive_words",
    uni_lemma == "bead" ~ "clothing",
    uni_lemma == "every" ~ "quantifiers",
    uni_lemma == "fishtank" ~ "household",
    uni_lemma == "he" ~ "pronouns",
    uni_lemma == "might" ~ "helping_verbs",
    .default = category)) |>
  mutate(uni_lemma = case_when(
    definition == "haber (hay)" ~ "have",
    definition == "musieť" ~ "must",
    uni_lemma == "Pencil" ~ "pencil", # Catalan typo
    uni_lemma == "allowed" ~ "allow",
    uni_lemma == "mop" ~ "mop (object)",
    uni_lemma == "her" ~ "3SG.POSS", # Dutch
    uni_lemma == "he" ~ "3SG", # Mandarin (Taiwanese)
    uni_lemma == "fishtank" ~ "fish tank",
    uni_lemma == "aggrieved" ~ "upset",
    uni_lemma == "aound" ~ "round", # Irish typo
    uni_lemma == "baby chair" ~ "high chair", # 
    uni_lemma == "self" & category == "vehicles" ~ "car", # Spanish (Chilean)
    uni_lemma == "self" & category == "pronouns" ~ "1SG", # Estonian
    uni_lemma == "back" & category == "body_parts" ~ "back (body part)", 
    uni_lemma == "nail" & category == "body_parts" ~ "fingernail", 
    uni_lemma == "baker" & category == "outside" ~ "bakery", 
    uni_lemma == "bat" & category == "toys" ~ "bat (object)", 
    uni_lemma == "chicken" & category == "animals" ~ "chicken (animal)", 
    uni_lemma == "I" ~ "1SG", # Japanese (boku, watashi)
    uni_lemma == "our" ~ "1PL.POSS",
    uni_lemma == "bead" ~ "beads",
    uni_lemma == "chewing gum" ~ "gum",
    uni_lemma == "clothing" ~ "clothes",
    uni_lemma == "cock-a-doodle-doo" ~ "cockadoodledoo",
    uni_lemma == "child's name" ~ "child's own name",
    uni_lemma == "fireman" ~ "firefighter",
    uni_lemma == "fridge/freezer" ~ "fridge",
    uni_lemma == "forger" ~ "forget",
    uni_lemma == "fries" ~ "french fries",
    .default = uni_lemma
  ))

xldf_clean <- xldf_clean |>
  mutate(category = case_when(
    uni_lemma == "have" & language == "Slovak" ~ "helping_verbs",
    definition == "musieť" ~ "helping_verbs",
    uni_lemma == "want" & language == "Slovak" ~ "helping_verbs",
    .default = category
  ))

prod_pars <- xldf_clean |> 
  arrange(language, uni_lemma, desc(a1)) |> # get most discriminating uni_lemma per lang
  select(uni_lemma, language, uid, category, language, d) |>
  group_by(uni_lemma, language) |>
  slice(1) |> 
  ungroup() |>
  left_join(syn_cats) |>
  rename(semantic_category = category,
         syntactic_category = lexical_category) |>
  select(-lexical_class)

save(prod_pars, xldf_clean, file=here::here("data/intermediates/xldf_clean_allforms.Rdata"))


gen_langs <- model_stats |> filter(participants < 300) |> pull(language)

train_langs <- setdiff(languages, gen_langs)
```

Get category proportions
```{r, eval=F}
# get modal semantic category per uni_lemma (xldf, or xldf_clean?)
category_counts <- xldf_clean %>%
  group_by(uni_lemma, category) %>%
  summarize(count = n(), .groups = 'drop')

modal_categories <- category_counts %>%
  group_by(uni_lemma) %>%
  filter(count == max(count)) %>%
  ungroup()

## We use canonical categories from the American English form
main_cats <- xldf_clean |> filter(language == "English (American)") |> pull(category) |> unique()
cat_props <- xldf_clean |> 
  filter(category %in% main_cats,
         language %in% train_langs) |> 
  group_by(language) |> 
  count(category) |> 
  mutate(prop = n / sum(n)) |> 
  group_by(category) |> 
  summarise(mean_prop = mean(prop))
write_csv(cat_props, "data/category_proportions.csv")

syn_cats <- read_csv("data/categories.csv",
                     col_names = c("category", "lexical_category", "lexical_class")) |> 
  distinct()
syn_props <- xldf |> 
  filter(language %in% train_langs) |> 
  left_join(syn_cats, by = join_by(category), relationship = "many-to-one") |> 
  group_by(language) |> 
  count(lexical_category) |> 
  filter(!is.na(lexical_category),
         lexical_category != "unknown") |> 
  mutate(prop = n / sum(n)) |>
  group_by(lexical_category) |> 
  summarise(mean_prop = mean(prop))
write_csv(syn_props, "data/syntactic_proportions.csv")
```

# Cross-validation

```{r}
category_props <- read_csv("data/category_proportions.csv") 
syntactic_props <- read_csv("data/syntactic_proportions.csv") |>
  rename(category = lexical_category)
```



```{r, eval=F}
all_fscores <- readRDS("data/intermediates/full_fscores_allforms.rds")


syn_cats <- read_csv("data/categories.csv",
                     col_names = c("category", "lexical_category", "lexical_class"))
# join syn_cats to xldf ? no, we actually 

create_cors <- function(full_model, full_fscores, lang, prod_sum, prod_test, 
                        sublist_func, sublist_name, bins = NULL) {
  if (sublist_name=="Random") {
    lapply(2:(length(train_langs)-1), \(k) {
      sapply(1:N_RAND, \(comp) {
        sublist <- sublist_func(prod_sum, S_LEN, k)
        if (is.element(k, BEST_Ks)) { 
          fscore_cor <- tryCatch({
            get_fscore_cor(sublist, xldf_clean, lang, full_fscores)
          }, error = function(e) {
            message(glue("Error in get_fscore_cor for language {lang}, k {k}: {e$message}"))
            NA
          })
        } else {
          fscore_cor <- NA
        }
        c(get_difficulty_metrics(sublist, prod_test), fscore_cor)
      }) |> t() |> 
        `colnames<-`(c("num_overlap", "difficulty_cor", "rmse", "fscore_cor")) |> 
        as_tibble() |> 
        mutate(run = 1:N_RAND, k = k)
    }) |> bind_rows() |> 
      mutate(language = lang, sublist = sublist_name)
  } else if(sublist_name=='syntactic' | sublist_name=='category') {
    if(sublist_name=="syntactic") { 
      cat_props = syntactic_props 
    } else {
      cat_props = category_props
    }
    sapply(2:(length(train_langs)-1), \(k) {
      sublist <- sublist_func(prod_sum, S_LEN, k, cat_props) 
      if (is.element(k, BEST_Ks)) { 
        fscore_cor <- tryCatch({
          get_fscore_cor(sublist, xldf_clean, lang, full_fscores)
        }, error = function(e) {
          message(glue("Error in get_fscore_cor for language {lang}, k {k}: {e$message}"))
          NA
        })
      } else {
        fscore_cor <- NA
      }
      c(get_difficulty_metrics(sublist, prod_test), fscore_cor) 
    }) |> t() |> 
      `colnames<-`(c("num_overlap", "difficulty_cor", "rmse", "fscore_cor")) |> 
      as_tibble() |> 
      mutate(run = NA, k = 2:(length(train_langs)-1), language = lang, sublist = sublist_name)
  } else {
    sapply(2:(length(train_langs)-1), \(k) {
      sublist <- if (is.null(bins)) sublist_func(prod_sum, S_LEN, k) else sublist_func(prod_sum, S_LEN, k, n_bins = bins)
      if (is.element(k, BEST_Ks)) { 
        fscore_cor <- tryCatch({
          get_fscore_cor(sublist, xldf_clean, lang, full_fscores)
        }, error = function(e) {
          message(glue("Error in get_fscore_cor for language {lang}, k {k}: {e$message}"))
          NA
        })
      } else {
        fscore_cor <- NA
      }
      c(get_difficulty_metrics(sublist, prod_test), fscore_cor)
    }) |> t() |> 
      `colnames<-`(c("num_overlap", "difficulty_cor", "rmse", "fscore_cor")) |> 
      as_tibble() |> 
      mutate(run = NA, k = 2:(length(train_langs)-1), language = lang, sublist = sublist_name)
  }
}

# needs to know whether to use syn_props or cat_props...
cv_res <- lapply(train_langs, \(lang) {
  message(glue("Calculating for {lang}..."))
  
  # semantic cats
  prod_semantic_cats <- prod_pars |> 
    filter(language %in% train_langs, language != lang) |> 
    group_by(uni_lemma) |> 
    count(semantic_category) |> 
    arrange(desc(semantic_category)) |> 
    slice(1)
  
  # syntactic cats
  prod_syntactic_cats <- prod_pars |> 
    filter(language %in% train_langs, language != lang) |> 
    group_by(uni_lemma) |> 
    count(syntactic_category) |> 
    arrange(desc(syntactic_category)) |> 
    slice(1)
  
  prod_semantic_sum <- prod_pars |> 
    filter(language %in% train_langs, language != lang) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm = TRUE),
              sd_d = sd(d, na.rm = TRUE)) |> 
    left_join(prod_semantic_cats |> select(-n), by = join_by(uni_lemma)) |>
    rename(category=semantic_category)
  
  prod_syntactic_sum <- prod_pars |> 
    filter(language %in% train_langs, language != lang) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm = TRUE),
              sd_d = sd(d, na.rm = TRUE)) |> 
    left_join(prod_syntactic_cats |> select(-n), by = join_by(uni_lemma)) |>
    rename(category=syntactic_category)
  
  prod_test <- prod_pars |> filter(language == lang)
  
  # calculate full fscores once (rather than once per comparison)
  full_model <- readRDS(glue("data/prod_models/{lang}_2PL_allforms_prod_fits.rds"))
  #full_fscores <- all_fscores[[lang]]
  full_fscores <- fscores(full_model$model, 
                          response.pattern = full_model$model@Data$data, 
                          method = "MAP")[,1] 
  
  cat_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                          make_proportional_sublist, "category")
  syn_cors <- create_cors(full_model, full_fscores, lang, prod_syntactic_sum, prod_test, 
                          make_proportional_sublist, "syntactic")
  prod_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_swadesh_sublist, "unstratified")
  bin2_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_binned_swadesh_sublist, "2 strata", bins = 2)
  bin3_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_binned_swadesh_sublist, "3 strata", bins = 3)
  bin4_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_binned_swadesh_sublist, "4 strata", bins = 4)
  bin5_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_binned_swadesh_sublist, "5 strata", bins = 5)
  rand_cors <- create_cors(full_model, full_fscores, lang, prod_semantic_sum, prod_test, 
                           make_random_sublist, "Random")
  
  bind_rows(prod_cors, bin2_cors, bin3_cors, bin4_cors, bin5_cors, cat_cors, syn_cors, rand_cors)
}) |> bind_rows()

cv_res_sum <- cv_res |> 
  group_by(k, language, sublist) |> 
  summarise(num_overlap = mean(num_overlap),
            difficulty_cor = mean(difficulty_cor),
            fscore_cor = mean(fscore_cor),
            rmse = mean(rmse)) |> 
  mutate(difficulty_cor_t = atanh(difficulty_cor), # Fisher transform
         fscore_cor_t = atanh(fscore_cor)) 

saveRDS(cv_res_sum, "data/intermediates/full_vs_swadesh_cv_fscores_allforms.rds")
```

```{r}
cv_res_sum <- readRDS("data/intermediates/full_vs_swadesh_cv_fscores_allforms.rds")

cv_output <- cv_res_sum |> 
  group_by(k, sublist) |> 
  summarise(num_overlap = mean(num_overlap, na.rm = TRUE),
            difficulty_cor = mean(difficulty_cor, na.rm = TRUE),
            rmse = mean(rmse, na.rm=T))

best_k <- cv_output |>
  ungroup() |> group_by(sublist) |>
  filter(difficulty_cor == max(difficulty_cor, na.rm = TRUE)) |>
  slice(1) |> ungroup() |> arrange(desc(difficulty_cor))
best_k |> kableExtra::kable(digits=3)
# k=26, 2 strata, num_overlap=93.2, diff_cor=.842
```

10/7/24: We believe we prefer difficulty correlation over RMSE since the samples (and thus language parameters) are not directly comparable (but should be correlated)

```{r, best-by-rmse, eval=F}
# selecting by RMSE doesn't favor high overlap (and k is less consistent)
best_k <- cv_output |>
  ungroup() |> group_by(sublist) |>
  filter(rmse == min(rmse, na.rm = TRUE)) |>
  slice(1) |> ungroup() |> arrange(rmse)
best_k # |> kableExtra::kable(digits=3)
```
Best RMSE (2 strata and unstratified k=25) still have pretty high overlap (~92.3), so maybe aren't bad candidates..? But their correlations are not great (~.84, maximum is 0.855).

```{r, fig.width=8, fig.height=8, include=F}
cv_output |> filter(rmse<1.2) |>
  ggplot(aes(x=rmse, y=difficulty_cor, color=num_overlap)) + 
  facet_wrap(. ~ sublist) +
  geom_point()
```



```{r, fig.width=10, fig.height=5}
cv_res_sum |>
  filter(is.element(sublist, c("Random","category","unstratified","syntactic","2 strata"))) |> 
  ggplot(aes(x = k, y = difficulty_cor, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  stat_summary(aes(group = interaction(k, sublist)), fun="median", position=position_dodge(.5)) + # maybe horizontal bar ?
  #geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Difficulty correlation") +
  theme(legend.position = "bottom") + theme_classic()
```

ToDo: not full boxplot, just show medians ?

```{r, fig.width=10, fig.height=5}
cv_res_sum |>
    #filter(is.element(sublist, c("Random","category","unstratified","2 strata"))) |> 
ggplot(aes(x = k, y = num_overlap, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Overlap") +
  theme_classic() + 
  theme(legend.position = "bottom") 
```

```{r, fig.width=9, fig.height=5}
t.test(subset(cv_res_sum, sublist=="unstratified" & k==23)$fscore_cor - 
       subset(cv_res_sum, sublist=="Random" & k==23)$fscore_cor) # n.s.
t.test(subset(cv_res_sum, sublist=="category" & k==25)$fscore_cor - 
       subset(cv_res_sum, sublist=="Random" & k==23)$fscore_cor) # n.s.
t.test(subset(cv_res_sum, sublist=="category" & k==25)$fscore_cor - 
       subset(cv_res_sum, sublist=="2 strata" & k==27)$fscore_cor) # n.s.
t.test(subset(cv_res_sum, sublist=="Random" & k==23)$fscore_cor - 
       subset(cv_res_sum, sublist=="2 strata" & k==27)$fscore_cor) # n.s.
t.test(subset(cv_res_sum, sublist=="syntactic" & k==27)$fscore_cor - 
       subset(cv_res_sum, sublist=="2 strata" & k==27)$fscore_cor) # n.s.
t.test(subset(cv_res_sum, sublist=="syntactic" & k==27)$fscore_cor - 
       subset(cv_res_sum, sublist=="category" & k==25)$fscore_cor) # n.s.

t.test(subset(cv_res_sum, sublist=="category" & k==25)$fscore_cor - 
       subset(cv_res_sum, sublist=="unstratified" & k==23)$fscore_cor) # p=.001 (category better)

t.test(subset(cv_res_sum, sublist=="unstratified" & k==23)$fscore_cor - 
       subset(cv_res_sum, sublist=="2 strata" & k==27)$fscore_cor) # p<.001 (2 strata .002 better)

t.test(subset(cv_res_sum, sublist=="syntactic" & k==23)$fscore_cor - 
       subset(cv_res_sum, sublist=="2 strata" & k==27)$fscore_cor) # p=.02 (s strata better)

# identify best k for each method
cv_res_sum$best_k = F
for(i in 1:nrow(best_k)) {
  idx <- which(cv_res_sum$k==best_k[i,]$k & cv_res_sum$sublist==best_k[i,]$sublist)
  cv_res_sum[idx,]$best_k = T
}

cv_res_sum |>
  filter(best_k==T) |>
  filter(sublist!="4 strata", sublist!="5 strata") |>
  ggplot(aes(x = language, y = fscore_cor, group = sublist, col = sublist)) +
  geom_point(alpha=.7) + # geom_jitter
  labs(y = "Fscore correlation") +
  theme(legend.position = "bottom") + theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust=0.95, vjust=0.9)) 
# TODO: just a table tallying for how many languages each method is the best?
```


CV results show that $k=27$ is the optimal value for the 2 difficulty strata method and the syntactic category-stratified method, $k=25$ is optimal for the semantic category-stratified method, and $k=23$ is optimal for the unstratified method.
We construct the sublist for each of these methods, and compare them in order to construct a final list for the generalization test.

# Generalization test
```{r}
all_prod <- readRDS("data/intermediates/allforms_prod_per_lang.rds")
```

```{r}

 # semantic cats
  prod_semantic_cats <- prod_pars |> 
    filter(language %in% train_langs) |> 
    group_by(uni_lemma) |> 
    count(semantic_category) |> 
    arrange(desc(semantic_category)) |> 
    slice(1)
  
  # syntactic cats
  prod_syntactic_cats <- prod_pars |> 
    filter(language %in% train_langs) |> 
    group_by(uni_lemma) |> 
    count(syntactic_category) |> 
    arrange(desc(syntactic_category)) |> 
    slice(1)
  
  prod_semantic_sum <- prod_pars |> 
    filter(language %in% train_langs) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm = TRUE),
              sd_d = sd(d, na.rm = TRUE)) |> 
    left_join(prod_semantic_cats |> select(-n), by = join_by(uni_lemma)) |>
    rename(category=semantic_category)
  
  prod_syntactic_sum <- prod_pars |> 
    filter(language %in% train_langs) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm = TRUE),
              sd_d = sd(d, na.rm = TRUE)) |> 
    left_join(prod_syntactic_cats |> select(-n), by = join_by(uni_lemma)) |>
    rename(category=syntactic_category)


# nrow(subset(prod_sum, num_langs>=26)) # 310 unilemmas on at least 26 forms
# nrow(subset(prod_sum, num_langs>=25)) # 342 on 25+
# nrow(subset(prod_sum, num_langs>=23)) # 392 on 23+
# random overlap: 
# length(intersect(sample(1:310, 100), sample(1:310, 100))) # ~31
# length(intersect(sample(1:392, 100), sample(1:392, 100)))


generate_sublist_cors <- function(sublist_function, sublist_name, this_k, prod_sum, bins = NULL) {
  if(sublist_name=="syntactic") {
    sublist <- sublist_function(prod_sum, S_LEN, this_k, syntactic_props)
  } else if(sublist_name=="category") {
    sublist <- sublist_function(prod_sum, S_LEN, this_k, category_props)
  } else if(is.null(bins)) {
    sublist <- sublist_function(prod_sum, S_LEN, this_k)
  } else {
    sublist <- sublist_function(prod_sum, S_LEN, this_k, bins)
  }
  
  cors <- sapply(gen_langs, \(lang) {
    get_sumscore_cor(sublist, xldf, all_prod, lang)
  }) |> t() |>
    `colnames<-`(c("num_overlap", "sumscore_cor")) |> 
    as_tibble(rownames = "language") |> 
    mutate(sublist = sublist_name)
  
  return(cors)
}
```

```{r, eval=F}
strat2_cors <- generate_sublist_cors(make_binned_swadesh_sublist, sublist_name = "2 strata", this_k = 27,
                                     prod_semantic_sum, bins = 2)

syntactic_cors <- generate_sublist_cors(make_proportional_sublist, sublist_name = "syntactic", this_k = 27,
                                        prod_syntactic_sum) 

category_cors <- generate_sublist_cors(make_proportional_sublist, sublist_name = "category", this_k = 25, 
                                       prod_semantic_sum)

unstrat_cors <- generate_sublist_cors(make_swadesh_sublist, sublist_name = "unstratified", this_k = 23,
                                      prod_semantic_sum)

# Random
rand_cors <- lapply(gen_langs, \(lang) {
  rand_cors <- sapply(1:N_RAND, \(comp) {
    sublist <- make_random_sublist(prod_sum, S_LEN, BEST_K)
    get_sumscore_cor(sublist, xldf, all_prod, lang)
  }) |> t() |> 
    `colnames<-`(c("num_overlap", "sumscore_cor")) |> 
    as_tibble() |> 
    mutate(run = 1:N_RAND,
           language = lang)
}) |> 
  bind_rows() |> 
  mutate(sublist = "Random")


# Combine all results
gen_res <- bind_rows(unstrat_cors, syntactic_cors, category_cors, rand_cors, strat2_cors)

gen_res_sum <- gen_res |> 
  group_by(language, sublist) |> 
  summarise(num_overlap = mean(num_overlap),
            sumscore_cor = mean(sumscore_cor)) |> 
  mutate(sumscore_cor_t = atanh(sumscore_cor)) # Fisher transform

save(gen_res_sum, gen_res, file="data/intermediates/generalization_results.rds")
```

```{r, generate-candidate-swadesh-lists}
load("data/intermediates/generalization_results.rds")

unstrat_sublist <- make_swadesh_sublist(prod_semantic_sum, S_LEN, 23)
category_sublist <- make_proportional_sublist(prod_semantic_sum, S_LEN, 25, category_props) 
syntactic_sublist <- make_proportional_sublist(prod_syntactic_sum, S_LEN, 27, syntactic_props) 
strat2_sublist <- make_binned_swadesh_sublist(prod_semantic_sum, S_LEN, 27, n_bins=2)

save(unstrat_sublist, category_sublist, syntactic_sublist, strat2_sublist,
     file="data/intermediates/best_k_swadesh_lists.rds")

table(unstrat_sublist$category) # 16 categories - few action words, more animals
table(category_sublist$category) # 21 cats - more actions, 
table(strat2_sublist$category) # 16 cats
table(syntactic_sublist$category) # ToDo: merge semantic category in..
# function_words          nouns          other     predicates 
#            13             46             16             25 

setdiff(unique(category_sublist$category), unique(unstrat_sublist$category))
# "quantifiers" "pronouns" "connecting_words" "helping_verbs" "furniture_rooms" 


# 62-96% overlap between the various methods
intersect(strat2_sublist$uni_lemma, category_sublist$uni_lemma) # 62
intersect(unstrat_sublist$uni_lemma, category_sublist$uni_lemma) # 62
intersect(unstrat_sublist$uni_lemma, strat2_sublist$uni_lemma) # 85
intersect(unstrat_sublist$uni_lemma, syntactic_sublist$uni_lemma) # 80
intersect(category_sublist$uni_lemma, syntactic_sublist$uni_lemma) # 65

intersect(category_sublist$uni_lemma, intersect(unstrat_sublist$uni_lemma, strat2_sublist$uni_lemma)) # 58

setdiff(unstrat_sublist$uni_lemma, category_sublist$uni_lemma) # a lot of extra sounds and animals
setdiff(category_sublist$uni_lemma, unstrat_sublist$uni_lemma)

setdiff(category_sublist$uni_lemma, strat2_sublist$uni_lemma)
setdiff(strat2_sublist$uni_lemma, category_sublist$uni_lemma)

# ToDo: make Venn diagram of overlap between methods


require(VennDiagram)
library(RColorBrewer)

venn.diagram(
  x = list(unstrat_sublist$uni_lemma, 
           strat2_sublist$uni_lemma, 
           category_sublist$uni_lemma,
           syntactic_sublist$uni_lemma),
  fill=brewer.pal(4, "Pastel2"),
  category.names = c("Unstratified" , "2 strata" , "Semantic", "Syntactic"),
  cat.cex = 1.1,
  cat.fontface = "bold",
  cex = 1.2,
  fontface = "bold",
  filename = 'Swadesh_venn_diagram.png',
  output=TRUE
)

```


Weighted vote (by difficulty correlation) on which items should be included on final Swadesh list

```{r}
# top 4 methods -- that is, the best of each class
all_swadesh <- bind_rows(category_sublist |> mutate(method = "category") |> select(-n, -wanted, -count, -mean_prop), 
                         unstrat_sublist |> mutate(method = "unstratified"),
                         syntactic_sublist |> mutate(method = "syntactic"),
                         strat2_sublist |> mutate(method = "2 strata") |> select(-bin)) |>
  rename(semantic_category = category) |>
  left_join(best_k, by=c("method"="sublist"))
  #left_join(uni_syn_cats |> select(uni_lemma, syntactic_category))

all_swadesh <- all_swadesh |> group_by(uni_lemma) |> # , semantic_category, num_langs
  summarise(weighted_votes = sum(difficulty_cor)) |>
  arrange(desc(weighted_votes))

View(all_swadesh)
write_csv(all_swadesh, file="weighted_votes_swadesh_list.csv")
```

ToDo: run generalization test again on final Swadesh list..? (vs. equal-length random...?)
(and only report generalization language results for this final (weighted-vote) Swadesh list vs random)


Difficulty distribution by method (and vs. overall)

```{r}

```



```{r, fig.width=7, fig.height=4.5}
ggplot(gen_res_sum,
       aes(x = language, y = sumscore_cor, fill = sublist)) +
  geom_col(position = "dodge") + 
  labs(y = "Sumscore correlation") +
  theme_classic() +
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(fill="List type")
```

```{r, fig.width=7, fig.height=4.5}
ggplot(gen_res_sum,
       aes(x = language, y = num_overlap, fill = sublist)) +
  geom_col(position = "dodge") +
  labs(y = "Overlap size") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```





T-tests:
ToDo: make these ANOVAs...? 

The Swadesh sublist selection methods are not significantly different than random in terms of sumscore correlation:
```{r}
t.test(gen_res_sum |> filter(sublist == "unstratified") |> pull(sumscore_cor),
       gen_res_sum |> filter(sublist == "Random") |> pull(sumscore_cor),
       paired = TRUE)

t.test(gen_res_sum |> filter(sublist == "2 strata") |> pull(sumscore_cor),
       gen_res_sum |> filter(sublist == "Random") |> pull(sumscore_cor),
       paired = TRUE)

t.test(gen_res_sum |> filter(sublist == "category") |> pull(sumscore_cor),
       gen_res_sum |> filter(sublist == "Random") |> pull(sumscore_cor),
       paired = TRUE)
```


Amount of overlap - unstratified no different than random
```{r}
t.test(gen_res_sum |> filter(sublist == "unstratified") |> pull(num_overlap),
       gen_res_sum |> filter(sublist == "Random") |> pull(num_overlap),
       paired = TRUE)
```

Category stratified has less overlap than random sublists
```{r}
t.test(gen_res_sum |> filter(sublist == "category") |> pull(num_overlap),
       gen_res_sum |> filter(sublist == "Random") |> pull(num_overlap),
       paired = TRUE)
```

2 difficulty strata is not significantly different in overlap than random
```{r}
t.test(gen_res_sum |> filter(sublist == "2 strata") |> pull(num_overlap),
       gen_res_sum |> filter(sublist == "Random") |> pull(num_overlap),
       paired = TRUE)
```


All of the methods result in sublists that are significantly easier than all items
```{r}
t.test(unstrat_sublist$mean_d,
       prod_sum$mean_d)
```


```{r}
t.test(strat2_sublist$mean_d,
       prod_sum$mean_d)
```

```{r}
t.test(category_sublist$mean_d,
       prod_sum$mean_d)
```

## Examining list composition

```{r}
swad_cats <- unstrat_sublist |> mutate(sublist = "unstratified") |>
  bind_rows(category_sublist |> mutate(sublist = "category")) |>
  bind_rows(strat2_sublist |> mutate(sublist = "2 strat")) |>
  group_by(sublist, category) %>% summarise(n = n()) %>% arrange(desc(n)) %>%
  mutate(freq = n / sum(n))

ggplot(swad_cats, aes(x = sublist, y = n, fill = category)) +
  geom_bar(stat = "identity", position = "stack") +
  theme_minimal()
```
so many animals! also other nouns (body parts, clothing, food/drink), action words, descriptives - what about relative to distribution of frequency on other forms?
average frequency per category on a CDI:WS


```{r}
unstrat_sublist |> mutate(sublist = "unstratified") |>
  bind_rows(category_sublist |> mutate(sublist = "semantic")) |>
  bind_rows(strat2_sublist |> mutate(sublist = "2 strata")) |>
  bind_rows(syntactic_sublist |> mutate(sublist = "syntactic")) |>
  bind_rows(prod_semantic_sum |> filter(num_langs>=BEST_K) |> mutate(sublist = "All (k>=25)")) |>
  ggplot(aes(x=sublist, y=mean_d)) + 
  geom_violin() + geom_jitter(alpha=.3, width=.1) + theme_classic()
```


```{r, fig.width=5, fig.height=12}

# look into NAs...kind of a lot of 'em

cat_freq <- xldf %>% # @Alvin filter to canonical categories?
  filter(#!is.na(category), 
         category!="locations_quantities_adverbs",
         category!="sounds2",
         category!="unknown") %>%
  group_by(category, language) %>%
  summarise(n = n()) %>%
  group_by(language) %>%
  mutate(freq = n / sum(n))

cat_freq %>% ggplot(aes(x=freq, y=category, group=language)) +
  geom_point(alpha=.4) +
  geom_point(data=swad_cats, aes(y=reorder(category, freq), color="red")) +
  theme_classic() + theme(legend.position="none")
```


is sd(d) predicted by variance in distributed semantic models across languages?
(animals -- and other concrete nouns -- but body partologies differ a lot cross-linguistically; and pronouns and prepositions even more)
- back uni-lemmas (or just use CDI definitions) and pick favorite multilingual LLM..(but prob can't get all languages)

- sd(d) vs. concreteness? (we know it's correlated with d..)


## Cross-linguistic similarities 

We look at the Spearman correlation between the item difficulty of each language compared to each other language. First, we look at similarities across all IRT parameters, and then we focus in on the Swadesh candidates.

```{r, echo=F, fig.width=8, fig.height=8}
get_xling_difficulty_similarity <- function(xldf) {
  languages <- unique(xldf$language)
  dif_cors <- matrix(0, nrow=length(languages), ncol=length(languages))
  dif_sims <- tibble()
  colnames(dif_cors) = languages
  rownames(dif_cors) = languages

  for(l1 in languages) {
    for(l2 in languages) {
      tmp <- xldf %>% filter(language==l1 | language==l2, !is.na(d)) %>%
        select(uni_lemma, category, category, language, d) %>% # uid, 
        group_by(uni_lemma, language) %>%
        slice(1) %>% 
        pivot_wider(names_from = language, values_from = d) %>%
        drop_na()
      dif_cors[l1,l2] <- cor(tmp[,l1], tmp[,l2], method="spearman")
      dif_sims <- bind_rows(dif_sims, tibble("Lang1" = l1, "Lang2" = l2, 
                                             "r" = cor(tmp[,l1], tmp[,l2], method="spearman")[[1]], 
                                             "N" = nrow(tmp)))
    }
  }
  return(dif_cors)
}

dif_cors <- get_xling_difficulty_similarity(xldf_clean)

Colors = brewer.pal(11,"Spectral")
diag(dif_cors) = NA
#bad_lang = c("Mandarin (Taiwanese)")
heatmap.2(dif_cors, col=Colors)
```

### Similarity in Swadesh item parameters

```{r, echo=F, fig.width=8, fig.height=8}
swad_dif_cors <- get_xling_difficulty_similarity(xldf_clean %>% 
                                              filter(is.element(uni_lemma, swadesh_sublist$uni_lemma)))

heatmap.2(swad_dif_cors, col=Colors)
```

## Baseline language similarity data

From [Bella, Batsuren, and Giunchiglia (2021)](http://ukc.disi.unitn.it/index.php/lexsim/).

```{r}
# citation:
# Gábor Bella, Khuyagbaatar Batsuren, and Fausto Giunchiglia. (2021). A Database and Visualization of the Similarity of Contemporary Lexicons. 24th International Conference on Text, Speech, and Dialogue. Olomouc, Czech Republic. Retrieved from http://ukc.disi.unitn.it/index.php/lexsim/ January 18, 2024.
lang_sims <- read.csv(file="data/similarities_1.0.tsv", sep='\t') 
# Similarity is a value between 0 and 100, confidence (Robustness) can be Low, Medium, or High. 
# The confidence of a result depends on the sizes of the lexicons over which the similarity value was computed: the smaller the lexicon sizes, the lower the confidence.

sort(unique(lang_sims$LangName_1))

# not existing: ASL, Estonian, BSL, Kiswahili, Kigiriama, 

str_split(language, " (")

matches = intersect(unique(lang_sims$LangName_1), languages) # 18
names(matches) = matches
setdiff(unique(xldf$lang_mapped), unique(lang_sims$LangName_1))


# they don't have: "American Sign Language","British Sign Language","Kiswahili" (or Swahili), "Kigiriama", Greek (Cypriot/not),
language_mapping <- c(matches, c(
  "Mandarin (Beijing)" = "Mandarin Chinese", "Mandarin (Taiwanese)" = "Mandarin Chinese",
  "Cantonese" = "Yue Chinese",
  "Norwegian" = "Norwegian Bokmål",
  #"Latvian" = "Latgalian", # is historical Latvian is close enough.. (still spoken in East Latvia..)
  "French (French)" = "French", "French (Quebecois)" = "French",
  "English (American)" = "English", "English (Australian)" = "English",
  "English (British)" = "English", "English (Irish)" = "English",
  "Portuguese (European)" = "Portuguese",
  "Spanish (Argentinian)" = "Spanish", "Spanish (Chilean)" = "Spanish",
  "Spanish (European)" = "Spanish", "Spanish (Mexican)" = "Spanish",
  "Spanish (Peruvian)" = "Spanish",
  "Greek (Cypriot)" = "Modern Greek",
  "Arabic (Saudi)" = "Standard Arabic" # or Egyptian / Moroccan ?
))
xldf <- xldf %>%
  mutate(lang_mapped = coalesce(language_mapping[language], language))

matches = intersect(unique(lang_sims$LangName_1), unique(xldf$lang_mapped)) # 26
langs_to_remove <- setdiff(unique(xldf$lang_mapped), unique(lang_sims$LangName_1))

# Replace NA values if the language doesn't have a mapping
xldf$lang_mapped[is.na(xldf$lang_mapped)] <- xldf$language[is.na(xldf$lang_mapped)]

lang_sims <- lang_sims %>%
  filter(is.element(LangName_1, language_mapping) & 
         is.element(LangName_2, language_mapping))

# table(lang_sims$Robustness) # mostly high confidence, a few medium

```

# Compare dendrogams of 

```{r}
# compare dendrograms: https://cran.r-project.org/web/packages/dendextend/vignettes/dendextend.html
require(dendextend)

lang_sims_long <- lang_sims %>% 
  bind_rows(lang_sims %>% mutate(orig_lang2 = LangName_2, 
                                 LangName_2 = LangName_1,
                                 LangName_1 = orig_lang2) %>% select(-orig_lang2)) %>%
  select(-ISO_1, -ISO_2, -Robustness)

lang_sims_mat <- lang_sims_long %>% 
  pivot_wider(values_from=Similarity, names_from = LangName_1) %>% as.data.frame()

rownames(lang_sims_mat) = lang_sims_mat$LangName_2
lang_sims_mat$LangName_2 = NULL

lang_sim_d <- dist(lang_sims_mat)
lang_sim_cl <- hclust(lang_sim_d, method = "average")
lang_sim_dend <- as.dendrogram(lang_sim_cl)

#pdf("lang_sim_dendrogram.pdf", height=100, width=10)
dendextend::plot_horiz.dendrogram(lang_sim_dend)
#dev.off()

# ToDo: do we need to use xldf_clean ?
swad_lang_sims_long <- xldf %>% 
  filter(!is.element(language, langs_to_remove)) %>%
  select(-language) %>% rename(language = lang_mapped) %>%
  filter(is.element(uni_lemma, swadesh_sublist$uni_lemma))
setdiff(unique(swad_lang_sims_long$language), names(lang_sims_mat)) # "Yue Chinese" ??
swad_dif_mat <- get_xling_difficulty_similarity(swad_lang_sims_long)

swad_d <- dist(swad_dif_mat)
swad_cl <- hclust(swad_d, method = "average")
swad_dend <- as.dendrogram(swad_cl)

dendextend::plot_horiz.dendrogram(swad_dend)


dend_diff(swad_dend, lang_sim_dend)
# more ways to distinguish differences: help(highlight_distinct_edges)
```
## Tanglegram!

- "unique" nodes are highlighted with dashed lines (i.e.: nodes which contains a combination of labels/items, which are not present in the other tree)
- connecting lines are colored to highlight two sub-trees which are present in both dendrograms (no overlap in our case..)


```{r}
dends <- dendlist(swad_dend, lang_sim_dend)

tanglegram(dends)
```

## Untangle


```{r}
dends %>% untangle(method = "step1side") %>% 
   tanglegram(common_subtrees_color_branches = TRUE)

# ToDo: some of the languages appear only in one column:
# setdiff(unique(lang_sims$LangName_2), unique(lang_sims$LangName_1))
#  "Yue Chinese"
# setdiff(unique(lang_sims$LangName_1), unique(lang_sims$LangName_2))
#  "Standard Arabic"
```

